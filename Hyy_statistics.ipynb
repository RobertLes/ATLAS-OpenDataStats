{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7CtkAXTSh3_"
   },
   "source": [
    "## ATLAS Open Data Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGqUO7iQSh4B"
   },
   "source": [
    "### To setup everytime\n",
    "We're going to be using a number of tools to help us:\n",
    "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
    "* awkward: lets us handle complex and nested data structures efficiently\n",
    "* numpy: provides numerical calculations such as histogramming\n",
    "* matplotlib: common tool for making plots, figures, images, visualisations\n",
    "* lmfit: tool for statistical fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PXnTKJOiSh4B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import uproot # for reading .root files\n",
    "import time # to measure time to analyse\n",
    "import math # for mathematical functions such as square root\n",
    "import awkward as ak # for handling complex and nested data structures efficiently\n",
    "import numpy as np # # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib.ticker import MaxNLocator,AutoMinorLocator # for minor ticks\n",
    "import lmfit # for the signal and background fits\n",
    "import vector #to use vectors\n",
    "\n",
    "import pyhf, json, cabinetry #likelihood model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9MszouWSh4B"
   },
   "source": [
    "## Example 1: Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0fmpWqmSh4B"
   },
   "source": [
    "We will use the [atlasopenmagic](https://opendata.atlas.cern/docs/data/atlasopenmagic) to access the open data directly from the ATLAS OpenData Portal so no need to download any samples. First we import the module and load the Open Data release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-b2rSCLSh4B",
    "outputId": "f1970dad-1fa5-4dad-9b3c-6b04a5825dbf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available releases:\n",
      "========================================\n",
      "2016e-8tev           2016 Open Data for education release of 8 TeV proton-proton collisions (https://opendata.cern.ch/record/3860).\n",
      "2020e-13tev          2020 Open Data for education release of 13 TeV proton-proton collisions (https://cern.ch/2r7xt).\n",
      "2024r-pp             2024 Open Data for research release for proton-proton collisions (https://opendata.cern.record/80020).\n",
      "2024r-hi             2024 Open Data for research release for heavy-ion collisions (https://opendata.cern.ch/record/80035).\n",
      "2025e-13tev-beta     2025 Open Data for education and outreach beta release for 13 TeV proton-proton collisions (https://opendata.cern.ch/record/93910).\n",
      "2025r-evgen-13tev    2025 Open Data for research release for event generation at 13 TeV (https://opendata.cern.ch/record/160000).\n",
      "2025r-evgen-13p6tev  2025 Open Data for research release for event generation at 13.6 TeV (https://opendata.cern.ch/record/160000).\n",
      "Fetching and caching all metadata for release: 2025e-13tev-beta...\n",
      "Fetched 374 datasets so far...\n",
      "Successfully cached 374 datasets.\n",
      "Active release: 2025e-13tev-beta. (Datasets path: REMOTE)\n"
     ]
    }
   ],
   "source": [
    "import atlasopenmagic as atom\n",
    "atom.available_releases()\n",
    "atom.set_release('2025e-13tev-beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fuB8PSq3Sh4E",
    "outputId": "f9190538-efe5-49a5-f836-686f2adfa7b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the skim to use for the analysis\n",
    "skim = \"GamGam\"\n",
    "\n",
    "# Let's get the list of files to go through\n",
    "# Notice that we use \"cache\" so that the files are downloaded locally and not streamed\n",
    "files_list = atom.get_urls('data', skim, protocol='https', cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZpEt22z5Sh4H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cut on the photon reconstruction quality\n",
    "def cut_photon_reconstruction(photon_isTightID):\n",
    "    # Only the events which have True for both photons are kept\n",
    "    return (photon_isTightID[:,0]==True) & (photon_isTightID[:,1]==True)\n",
    "\n",
    "# Cut on the transverse momentum\n",
    "def cut_photon_pt(photon_pt):\n",
    "# Only the events where photon_pt[0] > 50 GeV and photon_pt[1] > 30 GeV are kept\n",
    "    return (photon_pt[:,0] > 50) & (photon_pt[:,1] > 30)\n",
    "\n",
    "# Cut on the energy isolation\n",
    "def cut_isolation_pt(photon_ptcone20, photon_pt):\n",
    "# Only the events where the calorimeter isolation is less than 5.5% are kept\n",
    "    return ((photon_ptcone20[:,0]/photon_pt[:,0]) < 0.055) & ((photon_ptcone20[:,1]/photon_pt[:,1]) < 0.055)\n",
    "\n",
    "# Cut on the pseudorapidity in barrel/end-cap transition region\n",
    "def cut_photon_eta_transition(photon_eta):\n",
    "# Only the events where modulus of photon_eta is outside the range 1.37 to 1.52 are kept\n",
    "    condition_0 = (np.abs(photon_eta[:, 0]) < 1.52) | (np.abs(photon_eta[:, 0]) > 1.37)\n",
    "    condition_1 = (np.abs(photon_eta[:, 1]) < 1.52) | (np.abs(photon_eta[:, 1]) > 1.37)\n",
    "    return condition_0 & condition_1\n",
    "\n",
    "# This function calculates the invariant mass of the 2-photon state\n",
    "def calc_mass(photon_pt, photon_eta, photon_phi, photon_e):\n",
    "    p4 = vector.zip({\"pt\": photon_pt, \"eta\": photon_eta, \"phi\": photon_phi, \"e\": photon_e})\n",
    "    invariant_mass = (p4[:, 0] + p4[:, 1]).M # .M calculates the invariant mass\n",
    "    return invariant_mass\n",
    "\n",
    "# Cut on null diphoton invariant mass\n",
    "def cut_mass(invariant_mass):\n",
    "    return (invariant_mass != 0)\n",
    "\n",
    "# Cut on the pT relative to the invariant mass\n",
    "# Only the events where the invididual photon pT is larger than 35% of the invariant mass are kept\n",
    "def cut_iso_mass(photon_pt, invariant_mass):\n",
    "    return ((photon_pt[:,0]/invariant_mass) > 0.35) & ((photon_pt[:,1]/invariant_mass) > 0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWuQV5WGSh4I"
   },
   "source": [
    "## Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHO7QyURSh4I"
   },
   "source": [
    "For the final analysis, we'll begin by applying the cuts and calculating the invariant masses across all the data. Once that's done, we'll fit the data to uncover the Higgs boson peak. Let's kick things off by applying the cuts and calculating those invariant masses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E85MyMWQSh4I",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodD.GamGam.root (0/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodE.GamGam.root (1/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodF.GamGam.root (2/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodG.GamGam.root (3/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodH.GamGam.root (4/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodJ.GamGam.root (5/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodA.GamGam.root (6/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodB.GamGam.root (7/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodC.GamGam.root (8/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodD.GamGam.root (9/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodE.GamGam.root (10/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodF.GamGam.root (11/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodG.GamGam.root (12/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodI.GamGam.root (13/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodK.GamGam.root (14/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodL.GamGam.root (15/16)\n"
     ]
    }
   ],
   "source": [
    "# Controls the fraction of all the events analysed. All of the data is used by\n",
    "# default to run this analysis (implemented in the loop over the tree). Reduce\n",
    "# this if you want the code to run quicker. This can take several minutes to\n",
    "# process the entire dataset\n",
    "fraction = 1.0\n",
    "# Holder for the masses as we process the files\n",
    "sample_data = []\n",
    "variables = [\"photon_pt\",\"photon_eta\",\"photon_phi\",\"photon_e\",\"photon_isTightID\",\"photon_ptcone20\"]\n",
    "\n",
    "# Loop over all the files in our list\n",
    "for afile in files_list:\n",
    "\n",
    "    # Print which sample is being processed\n",
    "    print(f'Processing file {afile} ({files_list.index(afile)}/{len(files_list)})')\n",
    "\n",
    "    # Open file\n",
    "    tree = uproot.open(afile + \":analysis\")\n",
    "\n",
    "    numevents = tree.num_entries\n",
    "\n",
    "    # Perform the cuts for each data entry in the tree and calculate the invariant mass\n",
    "    for data in tree.iterate(variables, library=\"ak\", entry_stop=int(numevents*fraction)):\n",
    "\n",
    "        photon_isTightID = data['photon_isTightID']\n",
    "        data = data[cut_photon_reconstruction(photon_isTightID)]\n",
    "\n",
    "        photon_pt = data['photon_pt']\n",
    "        data = data[cut_photon_pt(photon_pt)]\n",
    "\n",
    "        data = data[cut_isolation_pt(data['photon_ptcone20'],data['photon_pt'])]\n",
    "\n",
    "        photon_eta = data['photon_eta']\n",
    "        data = data[cut_photon_eta_transition(photon_eta)]\n",
    "\n",
    "        data['mass'] = calc_mass(data['photon_pt'], data['photon_eta'], data['photon_phi'], data['photon_e'])\n",
    "\n",
    "        data = data[cut_mass(data['mass'])]\n",
    "\n",
    "        data = data[cut_iso_mass(data['photon_pt'], data['mass'])]\n",
    "\n",
    "        # Append data to the whole sample data list\n",
    "        sample_data.append(data['mass'])\n",
    "        \n",
    "# turns sample_data back into an awkward array\n",
    "all_data = ak.concatenate(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra4OaN9KSh4I"
   },
   "source": [
    "We are now ready to fit our data to effectively detect the Higgs boson! We will use a combination of a 4th order polynomial and a Gaussian function. The polynomial function represents the background, while the Gaussian function represents our signal. The Gaussian model is used to fit the signal due to the nature of the detector's resolution. The fourth-order polynomial is chosen for the background because it offers enough flexibility to capture the overall shape without overfitting, thereby reducing the influence of spurious data—random, irrelevant fluctuations or noise that do not correspond to the true signal or background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating a background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(bin_centres, data_x, mass=125, degree=3):\n",
    "    \n",
    "    # data fit\n",
    "    polynomial_mod = lmfit.models.PolynomialModel( degree ) # 4th order polynomial\n",
    "    gaussian_mod = lmfit.models.GaussianModel() # Gaussian\n",
    "\n",
    "    # set initial guesses for the parameters of the polynomial model\n",
    "    pars = polynomial_mod.guess(data_x, # data to use to guess parameter values\n",
    "                                x=bin_centres)\n",
    "    #for par in pars: pars[par].set(value=1)\n",
    "\n",
    "    # set initial guesses and ranges for the parameters of the Gaussian model\n",
    "    pars += gaussian_mod.make_params(center=dict(value=mass, vary=False),\n",
    "                       amplitude=dict(value=100, min=0, max=1e3),\n",
    "                       sigma=dict(value=2, min=1, max=5))\n",
    "\n",
    "    model = polynomial_mod + gaussian_mod # combined model\n",
    "\n",
    "    # fit the model to the data\n",
    "    result = model.fit(data_x, # data to be fit\n",
    "                    pars, # guesses for the parameters\n",
    "                    x=bin_centres, weights=1/data_x_errors ) \n",
    "    print(result.fit_report())\n",
    "\n",
    "    # background part of fit\n",
    "    params_dict = result.params # get the parameters from the fit to data\n",
    "    #for par in params_dict: print(f\"{par},\\t{params_dict[par].value},\\t{params_dict[par].stderr}\")\n",
    "    background_x=polynomial_mod.eval(params=params_dict,x=bin_centres)\n",
    "\n",
    "    # data fit - background fit = signal fit\n",
    "    signal_x = gaussian_mod.eval(params=params_dict,x=bin_centres)\n",
    "\n",
    "    # best fit result in total\n",
    "    #sb_x=result.best_fit\n",
    "    \n",
    "    # model uncertainity\n",
    "    # dely0 = result.eval_uncertainty(sigma=1,params=params_dict)\n",
    "    # print(dely0)\n",
    "    # from copy import deepcopy\n",
    "    # params_dict_bkg=deepcopy(params_dict)\n",
    "    # #for par in params_dict.keys():\n",
    "    # #    if par==\"amplitude\" or par==\"center\" or par==\"sigma\":\n",
    "    # #        params_dict_bkg.set(vary=False)\n",
    "    # #        print(par)\n",
    "    # for name in params_dict_bkg: params_dict_bkg[name].vary = (name not in [\"amplitude\", \"center\", \"sigma\",\"fwhm\",\"height\"])   \n",
    "    # for name in params_dict_bkg:\n",
    "    #     print(name,params_dict_bkg[name],params_dict_bkg[name].vary)\n",
    "    # print(params_dict_bkg)\n",
    "    # dely = result.eval_uncertainty(sigma=1,params=params_dict_bkg)\n",
    "    # print(dely-dely0)\n",
    "\n",
    "    model_x_errors = result.eval_uncertainty(sigma=1)\n",
    "    \n",
    "    return signal_x, background_x, model_x_errors, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAnalysis(bin_centres, data_x, data_x_errors, background_x, signal_x, model_x_errors):\n",
    "    # *************\n",
    "    # Main plot\n",
    "    # *************\n",
    "    plt.axes([0.1,0.3,0.85,0.65]) # left, bottom, width, height\n",
    "    main_axes = plt.gca() # get current axes\n",
    "\n",
    "    # plot the data points\n",
    "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors, fmt='ko', label='Data', markersize=4) # black circles\n",
    "\n",
    "    # plot the background only fit\n",
    "    main_axes.plot(bin_centres, background_x, '--b', label='Bkg (4th order polynomial)') # dashed blue line\n",
    "\n",
    "    plt.fill_between(bin_centres, background_x-model_x_errors, background_x+model_x_errors, color=\"gray\", label=r'1-$\\sigma$ uncertainty band')\n",
    "\n",
    "    # plot the signal + background fit\n",
    "    main_axes.plot(bin_centres, signal_x+background_x,'-r', label='Sig+Bkg Fit ($m_H=125$ GeV)') # single red line\n",
    "\n",
    "    # set the x-limit of the main axes\n",
    "    main_axes.set_xlim( left=xmin, right=xmax )\n",
    "\n",
    "    # separation of x-axis minor ticks\n",
    "    main_axes.xaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "    # set the axis tick parameters for the main axes\n",
    "    main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        labelbottom=False, # don't draw tick labels on bottom axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "    # write y-axis label for main\n",
    "    main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        horizontalalignment='right')\n",
    "\n",
    "    # set the y-axis limit for the main axes\n",
    "    main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.5 )\n",
    "\n",
    "    # set minor ticks on the y-axis of the main axes\n",
    "    main_axes.yaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "    # avoid displaying y=0 on the main axes\n",
    "    main_axes.yaxis.get_major_ticks()[0].set_visible(False)\n",
    "\n",
    "    # Add text 'ATLAS Open Data' on plot\n",
    "    plt.text(0.2, 0.92, 'ATLAS Open Data', transform=main_axes.transAxes, fontsize=13 )\n",
    "\n",
    "    # Add text 'for education' on plot\n",
    "    plt.text(0.2, 0.86, 'for education', transform=main_axes.transAxes, style='italic',fontsize=8 )\n",
    "\n",
    "    lumi = 36.1\n",
    "    lumi_used = str(lumi*fraction) \n",
    "    plt.text(0.2, 0.8, r'$\\sqrt{s}$=13 TeV,$\\int$L dt = '+lumi_used+' fb$^{-1}$', transform=main_axes.transAxes ) \n",
    "\n",
    "    # Add a label for the analysis carried out\n",
    "    plt.text(0.2, 0.74, r'$H \\rightarrow \\gamma\\gamma$', transform=main_axes.transAxes ) \n",
    "\n",
    "    # draw the legend\n",
    "    main_axes.legend(frameon=False, # no box around the legend\n",
    "                    loc='lower left' ) # legend location\n",
    "\n",
    "    # *************\n",
    "    # Data-Bkg plot\n",
    "    # *************\n",
    "    plt.axes([0.1,0.1,0.85,0.2]) # left, bottom, width, height\n",
    "    sub_axes = plt.gca() # get the current axes\n",
    "\n",
    "    # set the y axis to be symmetric about Data-Background=0\n",
    "    sub_axes.yaxis.set_major_locator( MaxNLocator(nbins='auto', symmetric=True) )\n",
    "\n",
    "    # plot Data-Background\n",
    "    sub_axes.errorbar(x=bin_centres, y=data_x-background_x, yerr=data_x_errors, fmt='ko',markersize=4 ) # black circles\n",
    "\n",
    "    # draw the background only fit\n",
    "    sub_axes.plot(bin_centres, background_x-background_x, '--b' )  # dashed blue line\n",
    "\n",
    "    # draw the fit to data\n",
    "    sub_axes.plot(bin_centres, signal_x, '-r' ) # single red line\n",
    "\n",
    "    plt.fill_between(bin_centres, -model_x_errors, model_x_errors, color=\"gray\", label=r'1-$\\sigma$ uncertainty band')\n",
    "\n",
    "    # set the x-axis limits on the sub axes\n",
    "    sub_axes.set_xlim( left=xmin, right=xmax )\n",
    "\n",
    "    # separation of x-axis minor ticks\n",
    "    sub_axes.xaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "    # x-axis label\n",
    "    sub_axes.set_xlabel(r'Di-photon invariant mass $\\mathrm{m_{\\gamma\\gamma}}$ [GeV]', x=1, horizontalalignment='right',fontsize=13 )\n",
    "\n",
    "    # set the tick parameters for the sub axes\n",
    "    sub_axes.tick_params(which='both', direction='in', top=True, right=True ) \n",
    "\n",
    "    # separation of y-axis minor ticks\n",
    "    sub_axes.yaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "    # y-axis label on the sub axes\n",
    "    sub_axes.set_ylabel( 'Events-Bkg' )\n",
    "\n",
    "    # Generic features for both plots\n",
    "    main_axes.yaxis.set_label_coords( -0.09, 1 ) # x,y coordinates of the y-axis label on the main axes\n",
    "    sub_axes.yaxis.set_label_coords( -0.09, 0.5 ) # x,y coordinates of the y-axis label on the sub axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "eByzAy0dSh4I",
    "outputId": "0e5a27bb-1a64-49cd-a74c-262a74e147fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bin edges\n",
    "xmin = 100 #GeV\n",
    "xmax = 160 #GeV\n",
    "step_size = 1 #GeV\n",
    "bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                    stop=xmax+step_size, # The interval doesn't include this value\n",
    "                    step=step_size ) # Spacing between values\n",
    "bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "\n",
    "#Set the data\n",
    "partial_data =all_data[::1]\n",
    "data_x,_ = np.histogram(ak.to_numpy(partial_data), bins=bin_edges ) # histogram the data\n",
    "data_x_errors = np.sqrt( data_x ) # statistical error on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal_x, background_x, model_x_errors,_=fitModel(bin_centres, data_x)\n",
    "plotAnalysis(bin_centres, data_x, data_x_errors, background_x, signal_x, model_x_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees=range(2,7)\n",
    "chi2s=[]\n",
    "dofs=[]\n",
    "\n",
    "for degree in degrees:\n",
    "    signal_x, background_x, model_x_errors, result=fitModel(bin_centres, data_x, degree=degree)\n",
    "    chi2s.append(result.chisqr)\n",
    "    dofs.append(result.nfree)\n",
    "\n",
    "plt.plot(degrees,chi2s)\n",
    "plt.plot(degrees,dofs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a likelihood mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePyhfConfig(data_x,signal_x,background_x,model_x_errors):\n",
    "    model_spec = {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"channel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"signal\",\n",
    "                        \"data\": signal_x.tolist(),\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"mu\", \"type\": \"normfactor\", \"data\":None}  # parameter of interest\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        \"data\": background_x.tolist(),\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"bkg_uncert\", \"type\": \"shapesys\", \"data\": model_x_errors.tolist()}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    # Build the model\n",
    "    # model = pyhf.Model(model_spec)\n",
    "    ## data = np.concatenate([data_x, model.config.auxdata])\n",
    "    # data = data_x.tolist()+model.config.auxdata\n",
    "\n",
    "    #The observed data must be given as a named observation in the channel\n",
    "    workspace_spec = {\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"channels\": model_spec[\"channels\"],\n",
    "        \"observations\": [\n",
    "            {\"name\": \"channel\", \"data\": data_x.tolist()}\n",
    "        ],\n",
    "        \"measurements\": [\n",
    "            {\n",
    "                \"name\": \"measurement\",\n",
    "                \"config\": {\n",
    "                    \"poi\": \"mu\",\n",
    "                    \"parameters\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return workspace_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_spec=makePyhfConfig(data_x,signal_x,background_x,model_x_errors)\n",
    "\n",
    "# Create the workspace object\n",
    "ws = pyhf.Workspace(workspace_spec)\n",
    "model = ws.model()       # same as before, created from workspace\n",
    "data = ws.data(model)   # observed data + auxdata automatically assembled\n",
    "\n",
    "print(f\"  POI name:   {model.config.poi_name}\")\n",
    "print(f\"  channels:   {model.config.channels}\")\n",
    "print(f\"     nbins:   {model.config.channel_nbins}\")\n",
    "print(f\"   samples:   {model.config.samples}\")\n",
    "print(f\" modifiers:   {model.config.modifiers}\")\n",
    "print(f\"N paramaters: {model.config.npars}\")\n",
    "print(f\"parameters:   {model.config.parameters}\")\n",
    "print(f\"  nauxdata:   {model.config.nauxdata}\")\n",
    "#print(f\"   auxdata:   {model.config.auxdata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefit = cabinetry.model_utils.prediction(model)\n",
    "_ = cabinetry.tabulate.yields(model_prefit, data)\n",
    "_ = cabinetry.visualize.data_mc(model_prefit, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfit = pyhf.infer.mle.fit(data, model)\n",
    "mu_hat = bestfit[model.config.poi_index]\n",
    "print(\"Best-fit μ:\", mu_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs = pyhf.infer.hypotest(1.0, data, model, test_stat=\"qtilde\")\n",
    "print(\"CLs:\", CLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_results = cabinetry.fit.significance(model, data)\n",
    "print(significance_results.observed_significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_x, background_x, model_x_errors,_ =fitModel(bin_centres, data_x, mass=140)\n",
    "plotAnalysis(bin_centres, data_x, data_x_errors, background_x, signal_x, model_x_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses=np.linspace(110,140,25)\n",
    "sigs=[]\n",
    "for mass in masses:\n",
    "    signal_x, background_x, model_x_errors,_=fitModel(bin_centres, data_x, mass=mass)\n",
    "    workspace_spec=makePyhfConfig(data_x,signal_x,background_x,model_x_errors)\n",
    "\n",
    "    # Create the workspace object\n",
    "    ws = pyhf.Workspace(workspace_spec)\n",
    "    model = ws.model()       # same as before, created from workspace\n",
    "    data = ws.data(model)   # observed data + auxdata automatically assembled\n",
    "    \n",
    "    significance_results = cabinetry.fit.significance(model, data)\n",
    "    print(significance_results.observed_significance)\n",
    "    sigs.append(significance_results.observed_significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(masses,sigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "OpenDataStatsTutorial",
   "language": "python",
   "name": "opendatastatstutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
