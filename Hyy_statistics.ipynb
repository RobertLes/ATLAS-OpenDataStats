{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATLAS Open-Data Statistics Tutorial: Making a likelihood model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first part of the statistics tutorial using ATLAS open data:\n",
    "1) Using open-data to run an analysis, run background function fits, then make a likelihood model\n",
    "2) Re-using public likelihood models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginning part will follow closely the existing notebook to gather Higgs to diphoton data: https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HyyAnalysis.ipynb\n",
    "You can find more information on how to access the ATLAS open-dataset in that tutroial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can proceed directly to the second section, but still run these cells to collect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7CtkAXTSh3_"
   },
   "source": [
    "## Run the Higgs->diphoton analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost directly copied from the previous notebook:\n",
    "https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HyyAnalysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PXnTKJOiSh4B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import uproot # for reading .root files\n",
    "import time # to measure time to analyse\n",
    "import math # for mathematical functions such as square root\n",
    "import awkward as ak # for handling complex and nested data structures efficiently\n",
    "import numpy as np # # for numerical calculations such as histogramming\n",
    "import vector #to use vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-b2rSCLSh4B",
    "outputId": "f1970dad-1fa5-4dad-9b3c-6b04a5825dbf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and caching all metadata for release: 2025e-13tev-beta...\n",
      "Fetched 374 datasets so far...\n",
      "Successfully cached 374 datasets.\n",
      "Active release: 2025e-13tev-beta. (Datasets path: REMOTE)\n"
     ]
    }
   ],
   "source": [
    "import atlasopenmagic as atom\n",
    "atom.set_release('2025e-13tev-beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fuB8PSq3Sh4E",
    "outputId": "f9190538-efe5-49a5-f836-686f2adfa7b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the skim to use for the analysis\n",
    "skim = \"GamGam\"\n",
    "\n",
    "# Let's get the list of files to go through\n",
    "# Notice that we use \"cache\" so that the files are downloaded locally and not streamed\n",
    "files_list = atom.get_urls('data', skim, protocol='https', cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZpEt22z5Sh4H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cut on the photon reconstruction quality\n",
    "def cut_photon_reconstruction(photon_isTightID):\n",
    "    # Only the events which have True for both photons are kept\n",
    "    return (photon_isTightID[:,0]==True) & (photon_isTightID[:,1]==True)\n",
    "\n",
    "# Cut on the transverse momentum\n",
    "def cut_photon_pt(photon_pt):\n",
    "# Only the events where photon_pt[0] > 50 GeV and photon_pt[1] > 30 GeV are kept\n",
    "    return (photon_pt[:,0] > 50) & (photon_pt[:,1] > 30)\n",
    "\n",
    "# Cut on the energy isolation\n",
    "def cut_isolation_pt(photon_ptcone20, photon_pt):\n",
    "# Only the events where the calorimeter isolation is less than 5.5% are kept\n",
    "    return ((photon_ptcone20[:,0]/photon_pt[:,0]) < 0.055) & ((photon_ptcone20[:,1]/photon_pt[:,1]) < 0.055)\n",
    "\n",
    "# Cut on the pseudorapidity in barrel/end-cap transition region\n",
    "def cut_photon_eta_transition(photon_eta):\n",
    "# Only the events where modulus of photon_eta is outside the range 1.37 to 1.52 are kept\n",
    "    condition_0 = (np.abs(photon_eta[:, 0]) < 1.52) | (np.abs(photon_eta[:, 0]) > 1.37)\n",
    "    condition_1 = (np.abs(photon_eta[:, 1]) < 1.52) | (np.abs(photon_eta[:, 1]) > 1.37)\n",
    "    return condition_0 & condition_1\n",
    "\n",
    "# This function calculates the invariant mass of the 2-photon state\n",
    "def calc_mass(photon_pt, photon_eta, photon_phi, photon_e):\n",
    "    p4 = vector.zip({\"pt\": photon_pt, \"eta\": photon_eta, \"phi\": photon_phi, \"e\": photon_e})\n",
    "    invariant_mass = (p4[:, 0] + p4[:, 1]).M # .M calculates the invariant mass\n",
    "    return invariant_mass\n",
    "\n",
    "# Cut on null diphoton invariant mass\n",
    "def cut_mass(invariant_mass):\n",
    "    return (invariant_mass != 0)\n",
    "\n",
    "# Cut on the pT relative to the invariant mass\n",
    "# Only the events where the invididual photon pT is larger than 35% of the invariant mass are kept\n",
    "def cut_iso_mass(photon_pt, invariant_mass):\n",
    "    return ((photon_pt[:,0]/invariant_mass) > 0.35) & ((photon_pt[:,1]/invariant_mass) > 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E85MyMWQSh4I",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodD.GamGam.root (0/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodE.GamGam.root (1/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodF.GamGam.root (2/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodG.GamGam.root (3/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodH.GamGam.root (4/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data15_periodJ.GamGam.root (5/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodA.GamGam.root (6/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodB.GamGam.root (7/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodC.GamGam.root (8/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodD.GamGam.root (9/16)\n",
      "Processing file simplecache::https://opendata.cern.ch/eos/opendata/atlas/rucio/opendata/ODEO_FEB2025_v0_GamGam_data16_periodE.GamGam.root (10/16)\n"
     ]
    },
    {
     "ename": "FSTimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/aiohttp/streams.py:355\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[0;32m--> 355\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/implementations/http.py:272\u001b[0m, in \u001b[0;36mHTTPFileSystem._get_file\u001b[0;34m(self, rpath, lpath, chunk_size, callback, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m chunk:\n\u001b[0;32m--> 272\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mread(chunk_size)\n\u001b[1;32m    273\u001b[0m     outfile\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/aiohttp/streams.py:436\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_nowait(n)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/aiohttp/streams.py:354\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/aiohttp/helpers.py:713\u001b[0m, in \u001b[0;36mTimerContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc_val\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFSTimeoutError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mafile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiles_list\u001b[38;5;241m.\u001b[39mindex(afile)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Open file\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43muproot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mafile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m numevents \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mnum_entries\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Perform the cuts for each data entry in the tree and calculate the invariant mass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/uproot/reading.py:142\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file_path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m ):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a string, pathlib.Path, an object with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m methods, or a length-1 dict of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mfile_path: object_path}, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mReadOnlyFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43marray_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marray_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecompression_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompression_executor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpretation_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpretation_executor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m object_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mroot_directory\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/uproot/reading.py:573\u001b[0m, in \u001b[0;36mReadOnlyFile.__init__\u001b[0;34m(self, file_path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m _file_header_fields_big\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not enough to read the TFile header (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    568\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    569\u001b[0m             _file_header_fields_big\u001b[38;5;241m.\u001b[39msize,\n\u001b[1;32m    570\u001b[0m         )\n\u001b[1;32m    571\u001b[0m     )\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbegin_chunk_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach_memmap()\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_before_interpret()\n\u001b[1;32m    579\u001b[0m (\n\u001b[1;32m    580\u001b[0m     magic,\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fVersion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin_chunk, _file_header_fields_small, {}\n\u001b[1;32m    596\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/uproot/source/fsspec.py:117\u001b[0m, in \u001b[0;36mFSSpecSource.chunk\u001b[0;34m(self, start, stop)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_requested_chunks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_requested_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m stop \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m--> 117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m future \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mTrivialFuture(data)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m uproot\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mchunk\u001b[38;5;241m.\u001b[39mChunk(\u001b[38;5;28mself\u001b[39m, start, stop, future)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/implementations/cached.py:473\u001b[0m, in \u001b[0;36mCachingFileSystem.__getattribute__.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;66;03m# all the methods defined in this class. Note `open` here, since\u001b[39;00m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;66;03m# it calls `_open`, but is actually in superclass\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/spec.py:797\u001b[0m, in \u001b[0;36mAbstractFileSystem.cat_file\u001b[0;34m(self, path, start, end, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the content of a file\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;124;03mkwargs: passed to ``open()``.\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# explicitly set buffering off?\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    799\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/implementations/cached.py:473\u001b[0m, in \u001b[0;36mCachingFileSystem.__getattribute__.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;66;03m# all the methods defined in this class. Note `open` here, since\u001b[39;00m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;66;03m# it calls `_open`, but is actually in superclass\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/spec.py:1338\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1337\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1338\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/implementations/cached.py:473\u001b[0m, in \u001b[0;36mCachingFileSystem.__getattribute__.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;66;03m# all the methods defined in this class. Note `open` here, since\u001b[39;00m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;66;03m# it calls `_open`, but is actually in superclass\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/implementations/cached.py:940\u001b[0m, in \u001b[0;36mSimpleCacheFileSystem._open\u001b[0;34m(self, path, mode, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m             f2\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(path, mode)\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/open-data/lib/python3.10/site-packages/fsspec/asyn.py:101\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m return_result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# suppress asyncio.TimeoutError, raise FSTimeoutError\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n",
      "\u001b[0;31mFSTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Controls the fraction of all the events analysed. All of the data is used by\n",
    "# default to run this analysis (implemented in the loop over the tree). Reduce\n",
    "# this if you want the code to run quicker. This can take several minutes to\n",
    "# process the entire dataset\n",
    "fraction = 1.0\n",
    "\n",
    "# Holder for the masses as we process the files\n",
    "sample_data = []\n",
    "variables = [\"photon_pt\",\"photon_eta\",\"photon_phi\",\"photon_e\",\"photon_isTightID\",\"photon_ptcone20\"]\n",
    "\n",
    "# Loop over all the files in our list\n",
    "for afile in files_list:\n",
    "\n",
    "    # Print which sample is being processed\n",
    "    print(f'Processing file {afile} ({files_list.index(afile)}/{len(files_list)})')\n",
    "\n",
    "    # Open file\n",
    "    tree = uproot.open(afile + \":analysis\")\n",
    "\n",
    "    numevents = tree.num_entries\n",
    "\n",
    "    # Perform the cuts for each data entry in the tree and calculate the invariant mass\n",
    "    for data in tree.iterate(variables, library=\"ak\", entry_stop=int(numevents*fraction)):\n",
    "\n",
    "        photon_isTightID = data['photon_isTightID']\n",
    "        data = data[cut_photon_reconstruction(photon_isTightID)]\n",
    "\n",
    "        photon_pt = data['photon_pt']\n",
    "        data = data[cut_photon_pt(photon_pt)]\n",
    "\n",
    "        data = data[cut_isolation_pt(data['photon_ptcone20'],data['photon_pt'])]\n",
    "\n",
    "        photon_eta = data['photon_eta']\n",
    "        data = data[cut_photon_eta_transition(photon_eta)]\n",
    "\n",
    "        data['mass'] = calc_mass(data['photon_pt'], data['photon_eta'], data['photon_phi'], data['photon_e'])\n",
    "\n",
    "        data = data[cut_mass(data['mass'])]\n",
    "\n",
    "        data = data[cut_iso_mass(data['photon_pt'], data['mass'])]\n",
    "\n",
    "        # Append data to the whole sample data list\n",
    "        sample_data.append(data['mass'])\n",
    "        \n",
    "# turns sample_data back into an awkward array\n",
    "all_data = ak.concatenate(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating a background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now we have data! Unfortunately though the signal we want to see isn't the only thing there: there is also some backgrounds. In general there are 2 ways we evaluate the backgrounds in high-energy experiments:\n",
    "1) Through dedicated simulations for each of the potential background proceses, which we apply the same analysis selection to evaluate the number of backgrounds\n",
    "2) By evaluating the background directly in the data\n",
    "\n",
    "For the Higgs diphoton analysis option 2) was used. But if we are *estimating something from data*, this is a **statistical method!**. So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import some module for plotting, and we will use `lmfit` to do the fitting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib.ticker import MaxNLocator,AutoMinorLocator # for minor ticks\n",
    "import lmfit # for the signal and background fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a first plot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis range of the plot\n",
    "xmin = 100 #GeV\n",
    "xmax = 160 #GeV\n",
    "\n",
    "# Histogram bin setup\n",
    "step_size = 1 #GeV\n",
    "bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                    stop=xmax+step_size, # The interval doesn't include this value\n",
    "                    step=step_size ) # Spacing between values\n",
    "bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "\n",
    "# Creating histogram from data\n",
    "data_x,_ = np.histogram(ak.to_numpy(all_data),\n",
    "                        bins=bin_edges ) # histogram the data\n",
    "data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "\n",
    "# *************\n",
    "# Main plot\n",
    "# *************\n",
    "main_axes = plt.gca() # get current axes\n",
    "\n",
    "# plot the data points\n",
    "main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                    fmt='ko', # 'k' means black and 'o' is for circles\n",
    "                    label='Data')\n",
    "\n",
    "# set the x-limit of the main axes\n",
    "main_axes.set_xlim( left=xmin, right=xmax )\n",
    "\n",
    "# separation of x axis minor ticks\n",
    "main_axes.xaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "# set the axis tick parameters for the main axes\n",
    "main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "# x-axis label\n",
    "main_axes.set_xlabel(r'di-photon invariant mass $\\mathrm{m_{\\gamma\\gamma}}$ [GeV]',\n",
    "                    fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "# write y-axis label for main axes\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        y=1, horizontalalignment='right')\n",
    "\n",
    "# set y-axis limits for main axes\n",
    "main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.6 )\n",
    "\n",
    "# add minor ticks on y-axis for main axes\n",
    "main_axes.yaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "# draw the legend\n",
    "main_axes.legend( frameon=False ); # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it's not obvious there is some signal here, and we have lots of background we need to estimate first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's make a model to estimate our background and signal. We will do this via *least squares* fitting. For this we will need to define some functions we think our data will follow. We'll model our:\n",
    "- Background: As a polynomial function $p(x)=a_0+a_1 x+ a_2 x^2+ ...$\n",
    "- Signal: As a Gaussian peak $g(x)=Gauss(\\mu,\\sigma)$ with mass $\\mu$ and width $\\sigma$\n",
    "We'll treat our total model as a sum of these two functions multiplied by the number of events of each: $f(x)=N_{background}p(x)+N_{signal}g(x)$\n",
    "\n",
    "We will fix our mass to some specific value, specify the order of the polynomial then we will fit simultaneously all the background parameters, and the number of signal/background events by minimzing the least squares: $$\\sum_i \\frac{(y_i-f(x_i))^2}{\\sigma^2_i} $$\n",
    "where $x_i, y_i$ are the fitted data coordinates with an uncertainity $\\sigma_i$, and the fitted function is $f(x_i)$\n",
    "\n",
    "While this sounds complicated, there are many existing packages which run least square fits, and so after specifying the model, it is almost completely automated. For convenience we will actually define this as a function so we can play with things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(bin_centres, data_x, mass=125, degree=4):\n",
    "    \n",
    "    # Define the signal and background mopel\n",
    "    polynomial_mod = lmfit.models.PolynomialModel( degree ) # 4th order polynomial\n",
    "    gaussian_mod = lmfit.models.GaussianModel() # Gaussian\n",
    "\n",
    "    # set initial guesses for the parameters of the polynomial model\n",
    "    pars = polynomial_mod.guess(data_x, x=bin_centres) #use a pre-guess from the data\n",
    "\n",
    "    # set initial guesses and ranges for the parameters of the Gaussian model\n",
    "    pars += gaussian_mod.make_params(center=dict(value=mass, vary=False),\n",
    "                       amplitude=dict(value=100, min=0, max=1e4),\n",
    "                       sigma=dict(value=2, min=1, max=5))\n",
    "\n",
    "    # Combined model\n",
    "    model = polynomial_mod + gaussian_mod\n",
    "\n",
    "    # fit the model to the data, this is most of the work!\n",
    "    result = model.fit(data_x, # data to be fit\n",
    "                    pars, # guesses for the parameters\n",
    "                    x=bin_centres, weights=1/data_x_errors ) \n",
    "\n",
    "    \n",
    "    # This stores the result of the fit\n",
    "    params_dict = result.params # get the parameters from the fit to data\n",
    "\n",
    "    # background part of fit\n",
    "    background_x=polynomial_mod.eval(params=params_dict,x=bin_centres)\n",
    "\n",
    "    # signal part of the fit\n",
    "    signal_x = gaussian_mod.eval(params=params_dict,x=bin_centres)\n",
    "\n",
    "    # get the uncertainity of the fit\n",
    "    model_x_errors = result.eval_uncertainty(sigma=1)\n",
    "    \n",
    "    return signal_x, background_x, model_x_errors, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also make a function to plot these results, this is the similar to the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAnalysis(bin_centres, data_x, data_x_errors, background_x, signal_x, model_x_errors):\n",
    "    # *************\n",
    "    # Main plot\n",
    "    # *************\n",
    "    plt.axes([0.1,0.3,0.85,0.65]) # left, bottom, width, height\n",
    "    main_axes = plt.gca() # get current axes\n",
    "\n",
    "    # plot the data points\n",
    "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors, fmt='ko', label='Data', markersize=4) # black circles\n",
    "\n",
    "    # plot the background only fit\n",
    "    main_axes.plot(bin_centres, background_x, '--b', label='Bkg') # dashed blue line\n",
    "\n",
    "    plt.fill_between(bin_centres, background_x-model_x_errors, background_x+model_x_errors, color=\"gray\", label=r'1-$\\sigma$ uncertainty band')\n",
    "\n",
    "    # plot the signal + background fit\n",
    "    main_axes.plot(bin_centres, signal_x+background_x,'-r', label='Sig+Bkg') # single red line\n",
    "\n",
    "    # set the x-limit of the main axes\n",
    "    main_axes.set_xlim( left=xmin, right=xmax )\n",
    "\n",
    "    # separation of x-axis minor ticks\n",
    "    main_axes.xaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "    # set the axis tick parameters for the main axes\n",
    "    main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        labelbottom=False, # don't draw tick labels on bottom axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "    # write y-axis label for main\n",
    "    main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        horizontalalignment='right')\n",
    "\n",
    "    # set the y-axis limit for the main axes\n",
    "    main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.5 )\n",
    "\n",
    "    # set minor ticks on the y-axis of the main axes\n",
    "    main_axes.yaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "    # avoid displaying y=0 on the main axes\n",
    "    main_axes.yaxis.get_major_ticks()[0].set_visible(False)\n",
    "\n",
    "    # Add text 'ATLAS Open Data' on plot\n",
    "    plt.text(0.2, 0.92, 'ATLAS Open Data', transform=main_axes.transAxes, fontsize=13 )\n",
    "\n",
    "    # Add text 'for education' on plot\n",
    "    plt.text(0.2, 0.86, 'for education', transform=main_axes.transAxes, style='italic',fontsize=8 )\n",
    "\n",
    "    lumi = 36.1\n",
    "    lumi_used = str(lumi*fraction) \n",
    "    plt.text(0.2, 0.8, r'$\\sqrt{s}$=13 TeV,$\\int$L dt = '+lumi_used+' fb$^{-1}$', transform=main_axes.transAxes ) \n",
    "\n",
    "    # Add a label for the analysis carried out\n",
    "    plt.text(0.2, 0.74, r'$H \\rightarrow \\gamma\\gamma$', transform=main_axes.transAxes ) \n",
    "\n",
    "    # draw the legend\n",
    "    main_axes.legend(frameon=False, # no box around the legend\n",
    "                    loc='lower left' ) # legend location\n",
    "\n",
    "    # *************\n",
    "    # Data-Bkg plot\n",
    "    # *************\n",
    "    plt.axes([0.1,0.1,0.85,0.2]) # left, bottom, width, height\n",
    "    sub_axes = plt.gca() # get the current axes\n",
    "\n",
    "    # set the y axis to be symmetric about Data-Background=0\n",
    "    sub_axes.yaxis.set_major_locator( MaxNLocator(nbins='auto', symmetric=True) )\n",
    "\n",
    "    # plot Data-Background\n",
    "    sub_axes.errorbar(x=bin_centres, y=data_x-background_x, yerr=data_x_errors, fmt='ko',markersize=4 ) # black circles\n",
    "\n",
    "    # draw the background only fit\n",
    "    sub_axes.plot(bin_centres, background_x-background_x, '--b' )  # dashed blue line\n",
    "\n",
    "    # draw the fit to data\n",
    "    sub_axes.plot(bin_centres, signal_x, '-r' ) # single red line\n",
    "\n",
    "    plt.fill_between(bin_centres, -model_x_errors, model_x_errors, color=\"gray\", label=r'1-$\\sigma$ uncertainty band')\n",
    "\n",
    "    # set the x-axis limits on the sub axes\n",
    "    sub_axes.set_xlim( left=xmin, right=xmax )\n",
    "\n",
    "    # separation of x-axis minor ticks\n",
    "    sub_axes.xaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "    # x-axis label\n",
    "    sub_axes.set_xlabel(r'Di-photon invariant mass $\\mathrm{m_{\\gamma\\gamma}}$ [GeV]', x=1, horizontalalignment='right',fontsize=13 )\n",
    "\n",
    "    # set the tick parameters for the sub axes\n",
    "    sub_axes.tick_params(which='both', direction='in', top=True, right=True ) \n",
    "\n",
    "    # separation of y-axis minor ticks\n",
    "    sub_axes.yaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "    # y-axis label on the sub axes\n",
    "    sub_axes.set_ylabel( 'Events-Bkg' )\n",
    "\n",
    "    # Generic features for both plots\n",
    "    main_axes.yaxis.set_label_coords( -0.09, 1 ) # x,y coordinates of the y-axis label on the main axes\n",
    "    sub_axes.yaxis.set_label_coords( -0.09, 0.5 ) # x,y coordinates of the y-axis label on the sub axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's histogram the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "eByzAy0dSh4I",
    "outputId": "0e5a27bb-1a64-49cd-a74c-262a74e147fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bin edges\n",
    "xmin = 100 #GeV\n",
    "xmax = 160 #GeV\n",
    "step_size = 1 #GeV\n",
    "bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                    stop=xmax+step_size, # The interval doesn't include this value\n",
    "                    step=step_size ) # Spacing between values\n",
    "bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "\n",
    "data_x,_ = np.histogram(ak.to_numpy(all_data), bins=bin_edges ) # histogram the data\n",
    "data_x_errors = np.sqrt( data_x ) # statistical error on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal_x, background_x, model_x_errors, result=fitModel(bin_centres, data_x)\n",
    "print(result.fit_report()) #print alos lot's of fit information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that lmfit can output alot of interesting statistical information! We'll get back to some of these in a moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the quick plot of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAnalysis(bin_centres, data_x, data_x_errors, background_x, signal_x, model_x_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super nice! Here we chose a fourth degree polynomial, which seems to do a good job, but is that the best choice?\n",
    "\n",
    "We can run a goodness of fit test to evaluate how well we think our fit model matches the data. We can calulate the *reduced chi square* $$\\chi^2_\\nu=\\frac{\\chi^2}{\\nu}$$ \n",
    "where:\n",
    "$$\\chi^2=\\sum_i \\frac{(y_i-f(x_i))^2}{\\sigma^2_i}$$\n",
    "and $\\nu$ = degrees of freedom = number of fitted paramaters - number of fitted data points\n",
    "\n",
    "Ideally for a good fit you have $\\chi_\\nu^2\\sim1$. Larger/smaller values means you could be under/over-fitting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so to test this assumption let's re-run the fit for backgrounds of degrees 2 to 7. The `lmfit` package nicely automatically calculated the $\\chi^2$ and $\\nu$ values for you, so let's plot this scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees=range(2,7)\n",
    "chi2s=np.zeros(len(degrees))\n",
    "dofs=np.zeros(len(degrees))\n",
    "\n",
    "for ii,degree in enumerate(degrees):\n",
    "    signal_x, background_x, model_x_errors, result=fitModel(bin_centres, data_x, degree=degree)\n",
    "    chi2s[ii]=result.chisqr\n",
    "    dofs[ii]=result.nfree\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(degrees,chi2s/dofs)\n",
    "ax.set_ylabel(r'$\\chi^2/\\nu$')\n",
    "ax.set_xlabel('Degree of background polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So indeed a choice of degree 3 or 4 polynomial seems to be a decent enough choice\n",
    "\n",
    "There are more advanced statistical methods for choosing the minimal most descriptive model (e.x. F-tests), but that's a story for another day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a likelihood model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so now we have a background and signal template for the data. How do we start making actual stating statistical statements on whethere the data **is** truly consitent with a signal and not just background noice.\n",
    "\n",
    "In ATLAS we almost always run statistical tests using a template profiled likelihood approach (check the lecture for more details). In this approach we need several pieces:\n",
    "- A histogram of the data for each region\n",
    "- A histogram of the signal for each region\n",
    "- A histogram of the background for each region\n",
    "- Uncertiainites on the signal/background model which can also be histograms, or specific numbers like normalization uncertainities\n",
    "\n",
    "The core building block is that each bin of the histograms will be treated like a Poisson counting experiment $$Pois(x_i|\\mu s_i+b_i)$$\n",
    "where the $\\mu$ is the signal strength which is a core piece of the model. For $\\mu=0$ this is a background only model, while $\\mu=1$ is our signal+background model. It's so important we call it the paramater of interest (poi).\n",
    "\n",
    "For our case we have just the one signal region, the data/signal/background histogram, and lmfit also gave us the uncertainity it predicted on the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the `pyhf` package to build these likelihood models right in python. The contents are specified in a `json` format, so they are relatively human readable. We will also make use of `cabinetry` which is built on-top of `pyhf` and has some more options for visualization and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf, json, cabinetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so let's make the likelihood model. As before we will write is as a function so we can play around with things later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePyhfConfig(data_x,signal_x,background_x,model_x_errors):\n",
    "    #First we'll make a json file specifying the predictions\n",
    "    model_spec = {\n",
    "        #Add the region\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"region\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        # Add the signal to this region\n",
    "                        \"name\": \"signal\",\n",
    "                        \"data\": signal_x.tolist(),\n",
    "                        # parameter of interest which can scale up/down in the fit\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"mu\", \"type\": \"normfactor\", \"data\":None}  \n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        # Add the background to this region\n",
    "                        \"name\": \"background\",\n",
    "                        \"data\": background_x.tolist(),\n",
    "                        # Add the uncertainity to the background\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"bkg_uncert\", \"type\": \"shapesys\", \"data\": model_x_errors.tolist()}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "\n",
    "    #Then we make the full json which has the data\n",
    "    workspace_spec = {\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"channels\": model_spec[\"channels\"], #Add in the precictions from above \n",
    "        \"observations\": [\n",
    "            {\"name\": \"region\", \"data\": data_x.tolist()} #Add in the data\n",
    "        ],\n",
    "        \"measurements\": [ #This is to tell pyhf we will want to fit the \n",
    "            {\n",
    "                \"name\": \"measurement\", \n",
    "                \"config\": {\n",
    "                    \"poi\": \"mu\",\n",
    "                    \"parameters\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return workspace_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now let's tell pyhf to make the model+data (also called a \"workspace\" in jargon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workspace object\n",
    "workspace_spec=makePyhfConfig(data_x,signal_x,background_x,model_x_errors)\n",
    "ws = pyhf.Workspace(workspace_spec)\n",
    "\n",
    "model = ws.model()      # Extract just the likelihood model from the workspace\n",
    "data = ws.data(model)   # Extract just the data from the workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out some of the model info to get a feel of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  POI name:   {model.config.poi_name}\")\n",
    "print(f\"  channels:   {model.config.channels}\")\n",
    "print(f\"     nbins:   {model.config.channel_nbins}\")\n",
    "print(f\"   samples:   {model.config.samples}\")\n",
    "print(f\" modifiers:   {model.config.modifiers}\")\n",
    "print(f\"N paramaters: {model.config.npars}\")\n",
    "print(f\"parameters:   {model.config.parameters}\")\n",
    "print(f\"  nauxdata:   {model.config.nauxdata}\")\n",
    "#print(f\"   auxdata:   {model.config.auxdata}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in this model we have actually specified 61 paramaters: The single poi $\\mu$, and 60 independent background uncertainities for each of the 60 bins. These 60 also have matching auxillary data, aka the bins of the background uncertainity histogram we provided, which help the model know how far these paramaters can vary (see the lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `cabinetry` we can also make a quick plot of things. Unsuprisingly they look similar to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefit = cabinetry.model_utils.prediction(model)\n",
    "_ = cabinetry.tabulate.yields(model_prefit, data)\n",
    "_ = cabinetry.visualize.data_mc(model_prefit, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to the fun stuff! We can now fit this model to the data, and extract our signal strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "mu_hat = fit_results.bestfit[model.config.poi_index]\n",
    "mu_err = fit_results.uncertainty[model.config.poi_index]\n",
    "print(f\"Best-fit mu: {mu_hat} +/- {mu_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty simple isn't it. While there is lots of details in statistsica, the actual implementation of the methods is mostly automatic. The biggest point is specifying a good likelihood model in the first place (and the form of that model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run some hypothesis testing! It's just as easy. Let's calculate the background-only p-value, aka the \"signficance\" of the signal. As a reminder in high-energy physics we have the very high-bar that we only reject the background-only model when we have greater then a $5\\sigma$ significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_results = cabinetry.fit.significance(model, data)\n",
    "print(f\" Signficance={significance_results.observed_significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like even with this partial open-dataset we can be very confident that you need the Higgs signal to match the data! (We may have taken some short-cuts in evluating all the necessary sources of systematics, but you get the picture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this bring up another point, how confident are we that the Higgs signal is at 125GeV. Why not another mass point? Are we robust to this assumption?!\n",
    "\n",
    "Let's re-run the signal/background template evaluation from the first step now assuming a mass peak at 140 GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal_x, background_x, model_x_errors, result =fitModel(bin_centres, data_x, mass=140)\n",
    "plotAnalysis(bin_centres, data_x, data_x_errors, background_x, signal_x, model_x_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, doesn't seem likely. Let's re-run the true hypothesis test though to double-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workspace_spec=makePyhfConfig(data_x,signal_x,background_x,model_x_errors)\n",
    "ws = pyhf.Workspace(workspace_spec)\n",
    "significance_results = cabinetry.fit.significance(ws.model() , ws.data(ws.model()))\n",
    "print(f\" Signficance={significance_results.observed_significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed there is no significant value, and say the data is consistent with a background-only model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what masses could be compatible. Let's run a scan! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses=np.linspace(110,140,25)\n",
    "sigs=[]\n",
    "for mass in masses:\n",
    "    signal_x, background_x, model_x_errors,_=fitModel(bin_centres, data_x, mass=mass)\n",
    "    workspace_spec=makePyhfConfig(data_x,signal_x,background_x,model_x_errors)\n",
    "\n",
    "    # Create the workspace object\n",
    "    ws = pyhf.Workspace(workspace_spec)\n",
    "    model = ws.model()       # same as before, created from workspace\n",
    "    data = ws.data(model)   # observed data + auxdata automatically assembled\n",
    "    \n",
    "    significance_results = cabinetry.fit.significance(model, data)\n",
    "    print(f\" Mass={mass}, Signficance={significance_results.observed_significance}\")\n",
    "    sigs.append(significance_results.observed_significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(masses,sigs)\n",
    "ax.set_ylabel('Signal Significance')\n",
    "ax.set_xlabel('Signal mass [GeV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare this to the plot from the Higgs discovery paper which used less data, but more experimental channels https://doi.org/10.1016/j.physletb.2012.08.020 (Just mirror the y-axis)\n",
    "\n",
    "You can see that the data is favoring a mass at 125 GeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cds.cern.ch/record/1471031/files/disc3_0_sig_p0_zoom.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "OpenDataStatsTutorial",
   "language": "python",
   "name": "opendatastatstutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
